# Novel automatic evaluation for emotion modification as well as sync, and visual quality evaluation 
## generating videos from filelists for use in evaluation

There are two scripts for generating videos en-mass: `gen_videos_from_filelist.py` and `real_videos_inference.py`.
`gen_videos_from_filelist.py` can be used with LRS2 data and the provided test filelists. But for generating videos based on MEAD iputs we will use the `affect` mode in `real_videos_inference.py`. This allows both the audio and video of the input video to be used as input to the generator, allowing only the affect of the input to be changed.
```
python3 real_videos_inference.py --mode affect 
                                 --filelist test_filelists/mead/<source emotion>.txt
                                 --results_dir <path to where you want videos to go>
                                 --data_root <path to actual videos to input to generator model>
                                 --checkpoint_path <path to model weights>
                                 --face_res 96
                                 --min_frame_res 160
```
Additionally use the `--full_mask` flag if you are working with a model which utilizes the full masking strategy, and use the `--save_gt_frames` if you wish to save the individual frames of the ground truth data for use in the FID visual quality evaluation later.

The face_res automatically adjusts the face video resolution so that the detected face (as judged by the first video frame)
is approximately face_res size in its longest dimension. Meanwhile min_frame_res sets the minimum size that the smallest
dimension of the video can drop to as a result of adjusting for the face size.

## evaluation of emotion modification
Our approach makes use of [NISL2020](https://github.com/wtomin/A-Multitask-Solution-for-FAU-EXPR-VA) for pre-frame valence and arousal judgements which we then use to judge the impact of our emotion modification on these affect measures. We normalize the change in valence and arousal by the baseline change in human expressions of the source and destination emotions in MEAD dataset. Further details can be found in [our paper](../literature/ADGD_2021_Wav2Lip-emotion.pdf).

You will need to set up NISL2020 following the instructions in their repo and then get the average valence and arousal scores over all frames in each video for the generated videos as well as the source and destination emotion ground truth inputs and targets from the MEAD dataset. These will allow you to use the `aggregate_affects.py` script to compute the final normalized change in valence and arousal scores.

## Sync metrics
We utilize the LSE-D and LSE-C metrics proposed by Prajwal  et  al. (2020). They use the pre-trained syncnet model:
``` 
git clone https://github.com/joonson/syncnet_python.git 
```
Set up the pretrained models and dependencies per the instructions in the repo using a seperate python environment. If you are using the docker and python packages provided in the root of this repo you can use pipenv as follows:
```
cd syncnet_python
python3 -m pipenv install -r requirements.txt
python3 -m pipenv install tqdm
pipenv shell
sh download_model.sh
```

### Running the evaluation scripts:
Copy the evaluation scripts to the cloned repository.
```  
    cp ../scores_LSE/*.py ./
    cp ../scores_LSE/*.sh ./ 
```

To run sync evaluation on a directory of .mp4 videos generated on LRS or MEAD inputs, please run the following command:
```
python calculate_sync_scores.py --data_root <dir with generated .mp4 videos to evaluate>
```

# Evaluation of image quality using FID metric.

Please see [this thread](https://github.com/Rudrabha/Wav2Lip/issues/22#issuecomment-712825380) in the original authors repo and [this thread](https://github.com/jagnusson/Wav2Lip-Emotion/issues/2) in our repo for more detail this use of FID scores for video visual quality assesment. 

First install the metric
```
pip install pytorch-fid
```

Then to get the FID score between two sets of videos, we use the folders of cropped face images generated by the inference generation scripts
(note: the ground truth frames will only be saved when --save_gt_frames is enabled on these). The folders will be output in the results dir.
```
python -m pytorch_fid <dir with face crops from one condition> <dir with face crops from another condition> --device <your gpu such as cuda:0>
```

# Acknowledgements
Like Wav2Lip original, our sync and visual quality evaluation pipelines are based on two existing repositories. LSE metrics are based on the [syncnet_python](https://github.com/joonson/syncnet_python) repository and the FID score is based on [pytorch-fid](https://github.com/mseitzer/pytorch-fid) repository. Additionally our novel automatic evaluation for emotion modification makes use of the [NISL2020](https://github.com/wtomin/A-Multitask-Solution-for-FAU-EXPR-VA) repository. We thank the authors of these repositories for releasing their code!



